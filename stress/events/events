#!/usr/bin/env python
# -*- coding: utf-8 -*-

##***** BEGIN LICENSE BLOCK *****
##Version: MPL 1.1
##
##The contents of this file are subject to the Mozilla Public License Version
##1.1 (the "License"); you may not use this file except in compliance with
##the License. You may obtain a copy of the License at
##http:##www.mozilla.org/MPL/
##
##Software distributed under the License is distributed on an "AS IS" basis,
##WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
##for the specific language governing rights and limitations under the
##License.
##
##The Original Code is the AllegroGraph Java Client interface.
##
##The Original Code was written by Franz Inc.
##Copyright (C) 2006 Franz Inc.  All Rights Reserved.
##
##***** END LICENSE BLOCK *****

"""
Usage: events

An event store stress tests.
"""

from __future__ import with_statement
from Queue import Empty
try:
    from multiprocessing import Process, Queue
except:
    assert False, \
        "Use Python 2.6 or install http://pypi.python.org/pypi/multiprocessing/"
from contextlib import contextmanager
from datetime import datetime, timedelta
import locale, os, random, subprocess, sys, time, traceback

sys.path.append(os.path.join(os.getcwd(), '../../src2'))

try:
    from collections import namedtuple
except ImportError:
    from franz.openrdf.util.namedtuple import namedtuple
from franz.openrdf.sail.allegrographserver import AllegroGraphServer
from franz.openrdf.query.query import QueryLanguage
from franz.openrdf.repository.repository import Repository
from franz.openrdf.vocabulary import XMLSchema, RDF
from franz.openrdf.model import Literal, Statement, URI, ValueFactory

class Defaults:
    # The namespace
    NS = 'http://franz.com/events#';

    # Number of worker processes
    LOAD_WORKERS = 20
    QUERY_WORKERS = 8

    # Time to run queries in minutes
    QUERY_TIME = 10

    # Size of the Events
    EVENT_SIZE = 50

    # Events per commit in bulk load phase
    BULK_EVENTS = 250

    # Goal store size
    SIZE = (10**9)

    # The catalog name
    CATALOG = 'tests'

    # The repository name
    REPOSITORY = 'events_test'

    # The starting phase
    PHASE = 1

    # OPEN OR RENEW
    OPEN = False

    # Add mixed workload phase
    MIXED = False

# The program options
OPT = None

LOCALHOST = 'localhost'
AG_HOST = os.environ.get('AGRAPH_HOST', LOCALHOST)
AG_PORT = int(os.environ.get('AGRAPH_PORT', '10035'))
AG_USER = os.environ.get('AGRAPH_USER', 'test')
AG_PASSWORD = os.environ.get('AGRAPH_PASSWORD', 'xyzzy')
PROG = sys.argv[0]

def nonzero(value):
    return max(0.0001, value)

def trace(formatter, values=None):
    if values:
        formatter = locale.format_string(formatter, values, grouping=True)
    print formatter
    sys.stdout.flush()

def connect(access_mode=Repository.OPEN):
    """
    Connect is called to connect to a store.
    """
    server = AllegroGraphServer(AG_HOST, AG_PORT, AG_USER, AG_PASSWORD)
    catalog = server.openCatalog(OPT.CATALOG)
    repository = catalog.getRepository(OPT.REPOSITORY, access_mode)
    repository.initialize()
    if access_mode is Repository.RENEW:
        repository.registerDatatypeMapping(datatype=XMLSchema.DATETIME,
            nativeType='datetime')
        repository.registerDatatypeMapping(datatype=XMLSchema.DATE,
            nativeType='date')
        repository.registerDatatypeMapping(datatype=XMLSchema.INT,
            nativeType='int')
        repository.registerDatatypeMapping(datatype=XMLSchema.FLOAT,
            nativeType='float')
    return repository.getConnection()

# PredInfo takes a uri in ntriples format and a generator function
# for the object ntriple string
class PredInfo(namedtuple('PredInfo', 'uri generator')):
    __slots__ = ()

    def __new__(cls, uri, generator):
        assert callable(generator)

        if not isinstance(uri, URI):
            uri = URI(namespace=OPT.NS, localname=uri)
        uri = uri.toNTriples()
        
        return tuple.__new__(cls, (uri, generator)) 

# Define some generators
class RandomDate(object):
    def __init__(self, start, end):
        object.__init__(self)
        self.start = start
        self.end = end
        self.seconds = (end - start).days * 24 * 60 * 60

    def random(self):
        return self.start + timedelta(seconds=random.randrange(self.seconds))

BaselineRange = \
    RandomDate(datetime(2008, 1, 1, 0, 0, 0),
               datetime(2008, 2, 1, 0, 0, 0))
BulkRange = \
    RandomDate(datetime(2008, 2, 1, 0, 0, 0),
               datetime(2009, 1, 1, 0, 0, 0))
SmallCommitsRange = \
    RandomDate(datetime(2009, 1, 1, 0, 0, 0),
               datetime(2009, 2, 1, 0, 0, 0))
DeleteRangeOne = \
    RandomDate(BaselineRange.start,
               datetime(2008, 1, 16, 0, 0, 0))
DeleteRangeTwo = \
    RandomDate(datetime(2008, 1, 16, 0, 0, 0),
               BaselineRange.end)
FullDateRange = \
    RandomDate(datetime(2008, 1, 1, 0, 0, 0),
               datetime(2009, 2, 1, 0, 0, 0))

def random_datetime():
    fn = random_datetime
    return Literal(fn.range.random()).toNTriples()

random_datetime.range = BaselineRange

def random_date():
    fn = random_datetime
    return Literal(fn.range.random().date()).toNTriples()

def random_int():
    return Literal(random.randrange(2**31-1), XMLSchema.INT).toNTriples()

def random_name():
    fn = random_name
    return Literal(' '.join((random.choice(fn.firsts), random.choice(fn.lasts)))
                   ).toNTriples()

random_name.firsts = ( 'Sam', 'Bruce', 'Mandy', 'Stacy', 'Marcus', 'Susan',
    'Jason', 'Chris', 'Becky', 'Britney', 'David', 'Paul', 'Daniel', 'James',
    'Bradley', 'Amy', 'Tina', 'Brandy', 'Jessica', 'Mary', 'George', 'Jane' )
random_name.lasts = ( 'Smith', 'Jones', 'Flintstones', 'Rubble', 'Jetson',
    'Wayne', 'McFly', 'Stadtham', 'Lee', 'Chan', 'Brown', 'Quinn',
    'Henderson', 'Anderson', 'Roland' )

def random_direction():
    fn = random_direction
    return Literal(random.choice(fn.choices)).toNTriples()

random_direction.choices = ('Inbound', 'Outbound')

def random_bool():
    fn = random_bool
    return Literal(random.choice(fn.choices)).toNTriples()

random_bool.choices = (False, True)

def random_origin():
    fn = random_origin
    return Literal(random.choice(fn.origins)).toNTriples()

random_origin.origins = ('Call Center', 'Sales', 'Front Desk' )

def random_payoption():
    fn = random_payoption
    return Literal(random.choice(fn.options)).toNTriples()

random_payoption.options = ('Cash', 'Credit', 'Money Order')

def random_delivery():
    fn = random_delivery
    return Literal(random.choice(fn.types)).toNTriples()

random_delivery.types = ('Mail', 'FedEx', 'UPS', 'Truck Freight')

def random_money():
    return Literal(round(random.uniform(0.01, 10000.00), 2)).toNTriples()

def random_uri(prefix, limit):
    return URI(namespace=OPT.NS,
        localname='%s-%d' % (prefix, random.randrange(int(limit)))).toNTriples()

def random_customer():
    # Striving for 1000 ids per prefix (e.g. customer) when SIZE is 1 billion,
    # so by default there are SIZE/1000 ids
    return random_uri('Customer', OPT.SIZE/1000)

def random_account():
    return random_uri('Account', OPT.SIZE/1000)

def random_action():
    fn = random_action
    return Literal(random.choice(fn.actions)).toNTriples()

random_action.actions = ('Add', 'Modify')

def random_handling():
    fn = random_handling
    return Literal(random.choice(fn.handling)).toNTriples()

random_handling.handling = ('Nothing', 'Past Due Notice', 'Collections') 

def type_uri(label):
    return URI(namespace=OPT.NS, localname=label).toNTriples()

# The list of events to generate
EVENTS = None

def initialize_events():
    global EVENTS

    interaction = [
        PredInfo(RDF.TYPE, lambda: type_uri('CustomerInteraction')),
        PredInfo('EventTimeStamp', random_datetime),
        PredInfo('EntityId', random_int),
        PredInfo('OriginatingSystem', lambda: '"CRM"'),
        PredInfo('Agent', random_name),
        PredInfo('Direction', random_direction),
        PredInfo('DoneInOne', random_bool),
        PredInfo('EndDate', random_datetime),
        PredInfo('FeeBased', random_bool),
        PredInfo('InteractionId', random_int),
        PredInfo('Origin', random_origin),
        PredInfo('PayOption', random_payoption),
        PredInfo('ReasonLevel1', random_int),
        PredInfo('ReasonLevel2', random_int),
        PredInfo('OriginatingSystem', random_int),
        PredInfo('Result', random_int)]

    invoice = [
        PredInfo(RDF.TYPE, lambda: type_uri('Invoice')),
        PredInfo('EventTimeStamp', random_datetime),
        PredInfo('AccountId', random_account),
        PredInfo('OriginatingSystem', lambda: '"Invoicing"'), 
        PredInfo('DueDate', random_datetime),
        PredInfo('Action', random_action),
        PredInfo('TotalAmountDue', random_money),
        PredInfo('AmountDueHandling', random_handling),
        PredInfo('LegalInvoiceNumber', random_int),
        PredInfo('PreviousBalanceAmount', random_money),
        PredInfo('TotalFinanceActivites', random_money),
        PredInfo('BillDate', random_date),
        PredInfo('TotalUsageCharges', random_money),
        PredInfo('TotalRecurringCharges', random_money),
        PredInfo('TotalOneTimeCharges', random_money)]

    payment = [
        PredInfo(RDF.TYPE, lambda: type_uri('AccountPayment')),
        PredInfo('EventTimeStamp', random_datetime),
        PredInfo('AccountId', random_account),
        PredInfo('OriginatingSystem', lambda: '"Ordering"'),
        PredInfo('SubscriberId', random_customer),
        PredInfo('InvoiceId', random_int),
        PredInfo('PaymentAmount', random_money),
        PredInfo('OriginalAmount', random_money),
        PredInfo('AmountDue', random_money),
        PredInfo('DepositDate', random_datetime),
        PredInfo('PaymentType', random_payoption),
        PredInfo('OriginalPostedAmount', random_money),
        PredInfo('PaymentTarget', lambda: random_uri('PaymentTarget', 100)),
        PredInfo('DepositDesignation',
            lambda: random_uri('DepositDesignation', 100)),
        PredInfo('BusinessEntity', lambda: random_uri('BusinessUnit', 100))]

    purchase = [
        PredInfo(RDF.TYPE, lambda: type_uri('Purchase')),
        PredInfo('EventTimeStamp', random_datetime),
        PredInfo('AccountId', random_account),
        PredInfo('OriginatingSystem', lambda: '"Sales"'),
        PredInfo('PurchaseDate', random_datetime),
        PredInfo('PurchaseAmount', random_money),
        PredInfo('InvoiceID', random_int),
        PredInfo('ProductID', lambda: random_uri('Product', OPT.SIZE/1000)), 
        PredInfo('LegalInvoiceNumber', random_int),
        PredInfo('PreviousBalanceAmount', random_money),
        PredInfo('DeliveredVia', random_delivery),
        PredInfo('PaidVia', random_payoption),
        PredInfo('CCApprovalNumber', random_int),
        PredInfo('TotalRecurringCharges', random_money),
        PredInfo('TotalOneTimeCharges', random_money)]

    def pad_events(event):
        pred_format = event[0].generator()[:-1] + 'Pad-%d>'

        for i in range(len(event), OPT.EVENT_SIZE):
            event.append(PredInfo(URI(uri=pred_format % i), random_int))

        assert len(event) == OPT.EVENT_SIZE
        return event

    EVENTS = map(pad_events, [interaction, invoice, payment, purchase])

def random_event(conn, storage, index):
    event = random.choice(EVENTS)

    # A customer URI for the quad's graph
    customer = random_customer()

    # Groups the event triples via an anonymous node
    bnode = conn.createBNode().toNTriples()

    for info in event:
        storage[index] = (bnode, info.uri, info.generator(), customer)
        index += 1

    return index

def buggy_version():
    """There is a bug in Python versions <= 2.6.2"""
    return map(int, sys.version.split()[0].split('.')) <= [2, 6, 2]

if buggy_version():
    from multiprocessing.queues import JoinableQueue as BadJoinableQueue
    class JoinableQueue(BadJoinableQueue):
        def put(self, obj, block=True, timeout=None):
            assert not self._closed
            if not self._sem.acquire(block, timeout):
                raise Full

            self._notempty.acquire()
            self._cond.acquire()
            try:
                if self._thread is None:
                    self._start_thread()
                self._buffer.append(obj)
                self._unfinished_tasks.release()
                self._notempty.notify()
            finally:
                self._cond.release()
                self._notempty.release()
else:
    from multiprocessing import JoinableQueue

class LoadPhase:
    start, baseline, bulk, small_commits, query, delete_one,\
        delete_two, die = range(8)
    last = delete_one

class PhaseParameters(object):
    def __init__(self, date_range, events_in_commit, triples):
        object.__init__(self)
        self.date_range = date_range
        self.events_in_commit = events_in_commit
        self.triples = triples

    @property
    def commits(self):
        return int(self.triples / (self.events_in_commit * OPT.EVENT_SIZE))

    @property
    def commits_per_worker(self):
        return int(self.commits / OPT.LOAD_WORKERS)

# The Phase Parameters
PHASE_PARAMS = None

# The work queues for loading and querying
loadq = None
queryq = None

def load_events(proc_num):
    """
    load_files does the work of the child processes.
    """
    conn = None
    
    def dequeue():
        try:
            return loadq.get()
        except Empty:
            return None

    def load_phase(phase):
        params = PHASE_PARAMS[phase]
        random_datetime.range = params.date_range
        quads = [None]*(OPT.EVENT_SIZE*params.events_in_commit)

        status_size = 50 if params.events_in_commit == 1 else 25
        start_time = time.time()

        count = 0
        errors = 0
        for commit in range(params.commits_per_worker):
            index = 0
            for event in range(params.events_in_commit):
                index = random_event(conn, quads, index)

            if commit > 0 and commit % status_size == 0:
                end_time = time.time()
                tpc = OPT.EVENT_SIZE*params.events_in_commit
                trace('loader(%d) [%s]: Loading Status - %d triples loaded so '
                    'far at %s triples per commit (%f commits/sec, %f triples/'
                    'sec over last %d commits), %d loading errors.', (
                    proc_num, datetime.now(), count, tpc,
                    status_size/nonzero(end_time - start_time),
                    tpc*status_size/nonzero(end_time - start_time),
                    status_size,
                    errors))
                start_time = end_time

            try:
                conn.mini_repository.addStatements(quads)
                count += len(quads)
            except Exception:
                trace('loader(%d) [%s]: Error adding quads...', (
                    proc_num, datetime.now()))
                errors += 1
                traceback.print_exc()
            
        trace('loader(%d) [%s]: Loading done - %d triples at %s triples '
            'per commit, %d loading errors.', (proc_num, datetime.now(),
            count, OPT.EVENT_SIZE*params.events_in_commit, errors))
        sys.stdout.flush()
        loadq.task_done()

    def delete_phase(phase):
        timestamp = URI(namespace=OPT.NS, localname='EventTimeStamp'
            ).toNTriples()
        date_range = PHASE_PARAMS[phase].date_range
        start, end = date_range.start, date_range.end
        start_nt = Literal(start).toNTriples()
        end_nt = Literal(end).toNTriples()

        queryString = """
            (select0 (?event)
              (:count-only t)
              (q- ?event !%s (? !%s !%s))
              (lisp (delete-triples :s ?event)))""" % (
                     timestamp, start_nt, end_nt)

        try:
            events = conn.prepareTupleQuery(QueryLanguage.PROLOG, queryString
                ).evaluate(count=True)
        except Exception:
            events = 0
            trace('loader(%d) [%s]: Error deleting triples...\n%s', (
                proc_num, datetime.now(), queryString))
            traceback.print_exc()

        trace('loader(%d) [%s]: Found %d events (%d triples) to delete.', (
            proc_num, datetime.now(), events, events * OPT.EVENT_SIZE))
        
        loadq.task_done()

    def get_phase(expected):
        phase = dequeue()

        while phase not in expected:
            # Put it back
            loadq.put(phase)
            loadq.task_done()
            time.sleep(1)
            phase = dequeue()

        return phase

    with connect().session(True, max(3600, (OPT.QUERY_TIME+1)*60)) as conn:
        if OPT.PHASE <= LoadPhase.baseline:
            phase = get_phase([LoadPhase.baseline])
            load_phase(phase)
        
        if OPT.PHASE <= LoadPhase.bulk:
            phase = get_phase([LoadPhase.bulk])
            load_phase(phase)

        if OPT.PHASE <= LoadPhase.small_commits:
            phase = get_phase([LoadPhase.small_commits])
            load_phase(phase)

        if OPT.PHASE <= LoadPhase.die:
            phase = get_phase([LoadPhase.delete_one, LoadPhase.delete_two,
                LoadPhase.die])
            if phase in [LoadPhase.delete_one, LoadPhase.delete_two]:
                delete_phase(phase)
                phase = get_phase([LoadPhase.die])

        loadq.task_done()

    conn.close()

def query_events(proc_num, resultq):
    conn = connect()
    conn.openSession(True)

    def dequeue():
        try:
            return queryq.get_nowait()
        except Empty:
            return None

    timestamp = URI(namespace=OPT.NS, localname='EventTimeStamp').toNTriples()

    # Perform the query in prolog or sparql
    language = QueryLanguage.PROLOG if proc_num % 2 == 1 \
        else QueryLanguage.SPARQL

    def random_query():
        # Pick a random customer
        customer = random_customer()

        # Pick a random date range
        start, end = FullDateRange.random(), FullDateRange.random()
        if start > end:
            start, end = end, start

        start_nt = Literal(start).toNTriples()
        end_nt = Literal(end).toNTriples()

        if language is QueryLanguage.PROLOG:
            queryString = """
                  (select0 (?event ?pred ?obj)
                    (:use-planner nil)
                    (:reorder nil)
                    (q- ?event !%s (? !%s !%s) !%s)
                    (q- ?event ?pred ?obj !%s))""" % (
                        timestamp, start_nt, end_nt, customer, customer)
        else:
            queryString = """
                SELECT ?event ?pred ?obj {
                 GRAPH %s {
                   ?event %s ?time .
                   FILTER (?time <= %s)
                   FILTER (?time >= %s)
                 }
                 GRAPH %s {
                    ?event ?pred ?obj }
                }""" % (customer, timestamp, end_nt, start_nt, customer)
    
        try:
            # Actually pull the full results to the client, then just count them
            count = len(conn.prepareTupleQuery(language, queryString
                ).evaluate())
        except Exception:
            # During huge bulk deletions, some queries may be invalidated
            # and a error returned to indicate they should be rerun. Keep
            # track of it if this happens.
            trace('query(%d) [%s]: Error executing query:\n%s\n', (
                proc_num, datetime.now(), queryString))
            traceback.print_exc()
            count = -1

        return count

    status_size = 10
    start_time = the_time = time.time()
    sub_count = 0
    queries = 0
    count = 0
    restarts = 0

    # Run the query at least once
    stop = None
    while stop is None:
        result = random_query()
        if result >= 0:
            count += result
        else:
            restarts += 1

        queries += 1
        stop = dequeue()

        if queries % status_size == 0:
            end_time = time.time()
            sub_count = count - sub_count
            trace('query(%d) [%s]: Querying status - %d triple results '
                'returned for %d queries in %f seconds (%f queries/second, '
                '%f triples per query), %d queries aborted.', (proc_num,
                datetime.now(), sub_count, status_size,
                end_time-start_time, status_size/nonzero(end_time-start_time),
                sub_count/status_size, restarts))
            start_time = end_time
            sub_count = count
            
    the_time = nonzero(time.time() - the_time)
    trace('query(%d) [%s]: %s Querying done - %d triple results returned for %d '
        'queries in %f seconds (%f queries/second, %f triples per query), '
        ' %d queries aborted.', (proc_num, datetime.now(), language,
        count, queries, the_time, queries/the_time, count/queries, restarts))

    conn.closeSession()
    conn.close()
    resultq.put((queries, count, the_time))
    queryq.task_done()


@contextmanager
def monitor(phase):
    """
    Start and end the monitor for a phase.
    """
    try:
        subprocess.call(['./monitor.sh', 'start', phase])
    except OSError:
        pass
    yield
    try:
        subprocess.call(['./monitor.sh', 'end'])
    except OSError:
        pass

def main():
    """
    The parent main process.
    """
    global loadq, PHASE_PARAMS

    initialize_events()

    # Reduce the number of times we need to round-trip to the server
    # for blank nodes
    ValueFactory.BLANK_NODE_AMOUNT = OPT.BULK_EVENTS * 4

    # Initialize the Phase Parameters
    PHASE_PARAMS = [
        None,
        PhaseParameters(BaselineRange, 1, OPT.SIZE/10),
        PhaseParameters(BulkRange, OPT.BULK_EVENTS, (OPT.SIZE*9)/10),
        PhaseParameters(SmallCommitsRange, 1, OPT.SIZE/10),
        None,
        PhaseParameters(DeleteRangeOne, OPT.BULK_EVENTS, OPT.SIZE/10),
        PhaseParameters(DeleteRangeTwo, OPT.BULK_EVENTS, OPT.SIZE/10)]

    # Renew/Open the repository
    trace('%s [%s]: %s %s:%s.', (PROG, datetime.now(),
        "Opening" if OPT.OPEN else "Renewing", OPT.CATALOG, OPT.REPOSITORY))
    conn = connect(Repository.OPEN if OPT.OPEN else Repository.RENEW)
    triples = conn.size()

    trace('%s [%s]: Testing with %d loading, %d querying processes. '
        'Repository contains %d triples.', (
        PROG, datetime.now(), OPT.LOAD_WORKERS, OPT.QUERY_WORKERS, triples))
    
    # Create the work queue
    loadq = JoinableQueue(OPT.LOAD_WORKERS)

    # Start the loading processes
    for proc_num in range(OPT.LOAD_WORKERS):
        p = Process(target=load_events, args=(proc_num,))
        p.start()

    def load_phase(phase):
        params = PHASE_PARAMS[phase]
        triples_start = conn.size()
        phase_time = time.time()

        # Tell the processes what to do (We only need one deletion process)
        if phase != LoadPhase.delete_one:
            for proc_num in range(OPT.LOAD_WORKERS):
                loadq.put(phase)
        else:
            loadq.put(LoadPhase.delete_one)
            loadq.put(LoadPhase.delete_two)

        if phase == LoadPhase.last:
            for proc_num in range(OPT.LOAD_WORKERS):
                loadq.put(LoadPhase.die)

            # Signal that there is no more work for the queue
            loadq.close()

        # Wait for all the work to be completed
        loadq.join()

        triples_end = conn.size()
        triples = triples_end - triples_start
        phase_time = nonzero(time.time() - phase_time)
        commits = abs(triples/(params.events_in_commit*OPT.EVENT_SIZE))
        trace('%s [%s]: %d total triples processed in %f seconds '
            '(%f triples/second, %f commits/second). '
            'Store contains %d triples.', (
            PROG, datetime.now(), triples, phase_time, triples/phase_time,
            commits/phase_time, triples_end))

    @contextmanager
    def run_queries():
        global queryq
        queryq = JoinableQueue(OPT.QUERY_WORKERS)
        resultq = Queue(OPT.QUERY_WORKERS)

        # Start the query processes
        for proc_num in range(OPT.QUERY_WORKERS):
            p = Process(target=query_events, args=(proc_num, resultq))
            p.start()

        yield

        for proc_num in range(OPT.QUERY_WORKERS):
            queryq.put('Stop')

        # Signal that there is no more work for the queue
        queryq.close()
        queryq.join()

        queries = 0
        triples = 0
        phase_time = 0

        for proc_num in range(OPT.QUERY_WORKERS):
            result = resultq.get()
            queries += result[0]
            triples += result[1]
            phase_time = nonzero(max(phase_time, result[2]))

        trace('%s [%s]: %d total triples returned over %d queries in '
            '%f seconds (%f triples/second, %f queries/second, '
            '%f triples/query). ', (PROG, datetime.now(), triples, queries,
            phase_time, triples/phase_time, queries/phase_time,
            triples/queries))

    total_time = time.time()
    if OPT.PHASE <= LoadPhase.baseline:
        with monitor('phase-1'):
            trace('%s [%s]: Phase 1: Baseline %d triple commits.', (
                PROG, datetime.now(), OPT.EVENT_SIZE))
            load_phase(LoadPhase.baseline)

    if OPT.PHASE <= LoadPhase.bulk:
        with monitor('phase-2'):
            trace('%s [%s]: Phase 2: Grow store to about %d triples.', (
                PROG, datetime.now(), OPT.SIZE))
            load_phase(LoadPhase.bulk)

    if OPT.PHASE <= LoadPhase.small_commits:
        with monitor('phase-3'):
            trace('%s [%s]: Phase 3: Perform %d triple commits.',
                (PROG, datetime.now(), OPT.EVENT_SIZE))
            load_phase(LoadPhase.small_commits)

    if OPT.PHASE <= LoadPhase.query:
        with monitor('phase-4'):
            trace('%s [%s]: Phase 4: Perform customer/date range queries '
                'with %d processes for %d minutes.', (PROG, datetime.now(),
                OPT.QUERY_WORKERS, OPT.QUERY_TIME))
            with run_queries():
                time.sleep(OPT.QUERY_TIME*60)

    if OPT.PHASE <= LoadPhase.delete_one:
        with monitor('phase-5'):
            trace('%s [%s]: Phase 5: Shrink store by 1 month.', (
                PROG, datetime.now()))
            load_phase(LoadPhase.delete_one)
    
    # Display the results
    total_time = time.time() - total_time
    triples_end = conn.size()
    triples = triples_end - triples

    trace('%s [%s]: Test completed in %f total seconds - '
        'store contains %d triples (%d triples added/removed).',
        (PROG, datetime.now(), total_time, triples_end, triples))


    if OPT.MIXED:
        # Do the mixed workload
        def run_mixed():
            global SmallCommitsRange, DeleteRangeOne, DeleteRangeTwo, FullDateRange, loadq

            with monitor('phase-6'):
                trace('%s [%s]: Phase 6: Mix workload - adds, queries, '
                    'and deletes.', (PROG, datetime.now()))

                # Just do small commit range and delete phases on loaders
                OPT.PHASE = 3

                # Basically, we'll add 30 days at a time, while doing queries
                # over the whole range. At the end of each 30 days, we'll
                # delete the oldest 30 days, the do it all again.
                fifteen_days = timedelta(days=15)
                thirty_days = timedelta(days=30)

                i = 1
                while i:
                    trace('%s [%s]: Phase 6: Performing workload iteration %d.'
                        ' Store contains %d triples.',
                        (PROG, datetime.now(), i, conn.size()))
                    i += 1

                    # Setup dates
                    SmallCommitsRange = RandomDate(SmallCommitsRange.end,
                        SmallCommitsRange.end + thirty_days)
                    FullDateRange = RandomDate(DeleteRangeTwo.end,
                        SmallCommitsRange.end)
                    DeleteRangeOne = RandomDate(DeleteRangeTwo.end,
                        DeleteRangeTwo.end + fifteen_days)
                    DeleteRangeTwo = RandomDate(DeleteRangeOne.end,
                        DeleteRangeOne.end + fifteen_days)

                    PHASE_PARAMS[LoadPhase.small_commits].date_range = \
                        SmallCommitsRange
                    PHASE_PARAMS[LoadPhase.delete_one].date_range = \
                        DeleteRangeOne
                    PHASE_PARAMS[LoadPhase.delete_two].date_range = \
                        DeleteRangeTwo

                    trace('%s [%s]: Phase 6: Running queries from %s to %s.', (
                        PROG, datetime.now(), FullDateRange.start,
                        FullDateRange.end))

                    with run_queries():
                        # Create the work queue
                        loadq = JoinableQueue(OPT.LOAD_WORKERS)

                        # Start the loading processes
                        for proc_num in range(OPT.LOAD_WORKERS):
                            p = Process(target=load_events, args=(proc_num,))
                            p.start()

                        trace('%s [%s]: Phase 6: Perform %d triple commits '
                            'from %s to %s.', (PROG, datetime.now(),
                            OPT.EVENT_SIZE, SmallCommitsRange.start,
                            SmallCommitsRange.end))
                        load_phase(LoadPhase.small_commits)
                        trace('%s [%s]: Phase 6: Shrink store by 30 days '
                            '(%s-%s).', (PROG, datetime.now(),
                            DeleteRangeOne.start, DeleteRangeTwo.end))
                        load_phase(LoadPhase.delete_one)

        run_mixed()

    conn.close()

if __name__ == '__main__':
    from copy import copy
    from optparse import OptionParser, Option, OptionValueError

    locale.setlocale(locale.LC_ALL, '')

    usage = 'Usage: %prog [options]\n\n' \
        'Environment Variables Consulted:\n' \
        'AGRAPH_HOST [default=localhost]\n' \
        'AGRAPH_PORT [default=10035]\n' \
        'AGRAPH_USER [default=test]\n' \
        'AGRAPH_PASSWORD [default=xyzzy]'

    def check_human_size(option, opt, value):
        try:
            if value[-1] == 'm':
                value = locale.atof(value[:-1])*10**6
            elif value[-1] == 'b':
                value = locale.atof(value[:-1])*10**9
            elif value[-1] == 't':
                value = locale.atof(value[:-1])*10**12
            else:
                value = locale.atoi(value)

            return int(value)
        except ValueError:
            raise OptionValueError(
                "option %s: invalid human-readable size value: %r" % (opt, value))

    class EventsOption(Option):
        TYPES = Option.TYPES + ('human_size',)
        TYPE_CHECKER = copy(Option.TYPE_CHECKER)
        TYPE_CHECKER['human_size'] = check_human_size
    
    parser = OptionParser(option_class=EventsOption, usage=usage, version="%prog 1.0")
    parser.add_option('-s', '--size', default=Defaults.SIZE,
        type='human_size', dest='SIZE', metavar='SIZE',
        help='SIZE triple limit for bulk load (e.g. 10,000, 100m, 2b, 1.5t) [default=%default]')
    parser.add_option('-l', '--load', default=Defaults.LOAD_WORKERS,
        type='int', dest='LOAD_WORKERS', metavar='LOAD',
        help='use LOAD number of loading processes [default=%default]')
    parser.add_option('-q', '--query', default=Defaults.QUERY_WORKERS,
        type='int', dest='QUERY_WORKERS', metavar='QUERY',
        help='use QUERY number of querying processes [default=%default]')
    parser.add_option('-n', '--namespace', default=Defaults.NS,
        dest='NS', metavar='NAMESPACE',
        help='use NAMESPACE for the store\'s URIs [default=%default]')
    parser.add_option('-t', '--time', default=Defaults.QUERY_TIME,
        type='int', dest='QUERY_TIME', metavar='MINUTES',
        help='MINUTES time limit for query phase [default=%default]')
    parser.add_option('-e', '--event', default=Defaults.EVENT_SIZE,
        type='int', dest='EVENT_SIZE', metavar='EVENT_SIZE',
        help='each event will contain EVENT_SIZE number of triples '
            '[default=%default]')
    parser.add_option('-b', '--bulk', default=Defaults.BULK_EVENTS,
        type='int', dest='BULK_EVENTS', metavar='BULK_EVENTS',
        help='commit BULK_EVENTS number of events per commit during bulk load '
            '[default=%default]')
    parser.add_option('-c', '--catalog', default=Defaults.CATALOG,
        dest='CATALOG', metavar='CATALOG',
        help='CATALOG name on server [default=%default]')
    parser.add_option('-r', '--repository', default=Defaults.REPOSITORY,
        dest='REPOSITORY', metavar='REPOSITORY',
        help='REPOSITORY name in the CATALOG [default=%default]')
    parser.add_option('-p', '--phase', default=Defaults.PHASE,
        dest='PHASE', metavar='PHASE', type='int',
        help='Run the test starting at phase PHASE [default=%default]')
    parser.add_option('-o', '--open', default=Defaults.OPEN,
        dest='OPEN', metavar='OPEN', action="store_true",
        help='OPEN the repository instead of RENEWING it [default=%default]')
    parser.add_option('-m', '--mixed', default=Defaults.MIXED,
        dest='MIXED', metavar='MIXED', action="store_true",
        help='Add MIXED workload phase after the normal run " \
            "(runs until a killall python) [default=do not run]')

    options, args = parser.parse_args()
    OPT = options
    del args
    main()
